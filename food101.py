# -*- coding: utf-8 -*-
"""food101.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/180Gi6LNDQjw3qjK1HUqGjYDZdlJL-fju
"""

!unzip '/content/drive/MyDrive/food101/small_data.zip' -d '/content/drive/MyDrive'

"""importing paackages"""

from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Flatten, Dense
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator

"""-to ignore the warning signs

"""

import warnings
warnings.filterwarnings('ignore')

"""-copying the data from drive"""

train_data='/content/drive/MyDrive/small_data/train'
test_data='/content/drive/MyDrive/small_data/validation'

"""#Vgg16
------

importing transfer learning applications

VGG16
-----

inputshape=size of the image

weights=one of None (random initialization), "imagenet" (pre-training on ImageNet), or the path to the weights file to be loaded.

include_top=whether to include the fully connected layer
"""

from keras.applications.vgg16 import VGG16
vgg=VGG16(input_shape=[224,224,3],weights='imagenet',include_top=False)

"""as we are using pre trained model we can't update the weights of the layers and even if we do that it won't we pre trrained so we take each layers and set them no to get trained"""

for layer in vgg.layers:
  layer.trainable=False

"""vgg.output gives the output after applying the convo and pooling layers so we now flatten them and save it in a variable 'x' and after that we pass it to the output layer"""

x=Flatten()(vgg.output)
prediction=Dense(15,activation='softmax')(x)

"""now building a model by specifying inputs and outputs"""

model=Model(inputs=vgg.input,outputs=prediction)

"""compling the model

> The Adam optimizer to update the weights.


> The categorical crossentropy loss function to measure how well the model's predictions match the true labels.


> Accuracy as a metric to track the performance of the model during training.






"""

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

"""preparing image data for training and testing


*   rescale=1/255: This parameter rescales the pixel values to 0-1 by dividing by 255. This normalization step is crucial for faster convergence of the neural network during training.
*  zoom_range=0.2: This parameter allows for random zooming of images by up to 20%.


*   horizontal_flip=True: This parameter allows for random horizontal flipping of images, which helps the model generalize better by learning from both the original and flipped versions of the images.

**flow_from_directory

    This method takes a directory path and generates batches of augmented data (for training) or just normalized data (for testing)

*   target_size=(224,224): This parameter resizes all images to the specified size of 224x224 pixels


*   batch_size=32: This parameter defines the number of images to be yielded from the generator per batch during training. A batch size of 32 is common and helps in efficient training.
*  class_mode='categorical': This parameter indicates that the labels are categorical (i.e., there are more than two classes)





"""

train_datagen=ImageDataGenerator(rescale=1/255,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1/255)



training_set=train_datagen.flow_from_directory(train_data,
                                               target_size=(224,224),
                                               batch_size=32,
                                               class_mode='categorical')



testing_set=test_datagen.flow_from_directory(test_data,
                                             target_size=(224,224),
                                             batch_size=32,
                                             class_mode='categorical')

"""training the model"""

ep=model.fit(training_set,validation_data=testing_set,epochs=10)

import matplotlib.pyplot as plt
from skimage.io import imread
img=imread('/content/Apple-Pie-Recipe-Video.jpg')
plt.imshow(img)

from skimage.transform import resize
img=resize(img,(224,224,3))
img.shape

import numpy as np

# Reshape the image to add an extra dimension
img = np.expand_dims(img, axis=0)

# Predict the class of the image
new = model.predict(img)

# Print the prediction
print(new)

pred = model.predict(testing_set)

model.save('vgg16.keras')

"""#InceptionResNetv2
--------
"""

from keras.applications.inception_resnet_v2 import InceptionResNetV2
incep=InceptionResNetV2(include_top=False,weights="imagenet",input_shape=(229,229,3))

for layer in incep.layers:
  layer.trainable=False

x=Flatten()(incep.output)
prediction=Dense(15,activation='softmax')(x)

model2=Model(inputs=incep.input,outputs=prediction)

model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

train_datagen=ImageDataGenerator(rescale=1/255,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1/255)



training_set=train_datagen.flow_from_directory(train_data,
                                               target_size=(229,229),
                                               batch_size=32,
                                               class_mode='categorical')



testing_set=test_datagen.flow_from_directory(test_data,
                                             target_size=(229,229),
                                             batch_size=32,
                                             class_mode='categorical')

early_stopping2 = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True,verbose=1)
model_checkpoint2 = ModelCheckpoint('incep.keras', monitor='val_loss', save_best_only=True,verbose=1)

callback2 = [early_stopping2, model_checkpoint2]

ep=model2.fit(training_set,validation_data=testing_set,epochs=10,callbacks=callback2)

import pandas as pd
pd.DataFrame(ep.history).plot()

"""#Xecption
---------
"""

from keras.applications.xception import Xception
xcep=Xception(include_top=False, weights="imagenet",input_shape=(229,229,3))

for layer in xcep.layers:
  layer.trainable=False

x=Flatten()(xcep.output)
prediction=Dense(15,activation='softmax')(x)

model3=Model(inputs=xcep.input,outputs=prediction)

model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

train_datagen=ImageDataGenerator(rescale=1/255,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1/255)



training_set=train_datagen.flow_from_directory(train_data,
                                               target_size=(229,229),
                                               batch_size=32,
                                               class_mode='categorical')



testing_set=test_datagen.flow_from_directory(test_data,
                                             target_size=(229,229),
                                             batch_size=32,
                                             class_mode='categorical')

early_stopping3 = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True,verbose=1)
model_checkpoint3 = ModelCheckpoint('xcep.keras', monitor='val_loss', save_best_only=True,verbose=1)

callback3 = [early_stopping3, model_checkpoint3]

ep3=model3.fit(training_set,validation_data=testing_set,epochs=10,callbacks=callback3)

pred = model3.predict(testing_set)

pd.DataFrame(ep3.history).plot()

"""#NASNetLarge
----------
"""

from keras.applications.nasnet  import NASNetLarge
nas=NASNetLarge(input_shape=(224,224,3),include_top=False,weights="imagenet")

for layer in nas.layers:
  layer.trainable=False

x=Flatten()(nas.output)
prediction=Dense(15,activation='softmax')(x)

model4=Model(inputs=xcep.input,outputs=prediction)

model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

train_datagen=ImageDataGenerator(rescale=1/255,
                                 zoom_range=0.2,
                                 horizontal_flip=True)
test_datagen=ImageDataGenerator(rescale=1/255)



training_set=train_datagen.flow_from_directory(train_data,
                                               target_size=(229,229),
                                               batch_size=32,
                                               class_mode='categorical')



testing_set=test_datagen.flow_from_directory(test_data,
                                             target_size=(229,229),
                                             batch_size=32,
                                             class_mode='categorical')

early_stopping4 = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True,verbose=1)
model_checkpoint4 = ModelCheckpoint('nas.keras', monitor='val_loss', save_best_only=True,verbose=1)

callback4 = [early_stopping4, model_checkpoint4]

ep4=model4.fit(training_set,validation_data=testing_set,epochs=10,callbacks=callback4)